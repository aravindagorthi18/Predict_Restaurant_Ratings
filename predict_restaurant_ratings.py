# -*- coding: utf-8 -*-
"""Predict_Restaurant_Ratings

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1diI5N1VASES8d-5EzPCfmUsGHK1LxZRg
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset (assuming you have a CSV file)
# Replace 'restaurant_data.csv' with your actual file path
df = pd.read_csv('Dataset_Cognify.csv')

# Basic exploratory analysis
print(df.info())
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Handle missing values
# For numerical columns - use mean imputation
# For categorical columns - use most frequent value imputation

# Identify numeric and categorical columns
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = df.select_dtypes(include=['object']).columns.tolist()

# Remove the target variable from features
if 'aggregate_rating' in numeric_features:
    numeric_features.remove('aggregate_rating')

# Create preprocessing pipelines
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Split the data
X = df.drop('aggregate_rating', axis=1)
y = df['aggregate_rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# First, make sure your data is loaded
df = pd.read_csv('Dataset_Cognify.csv')  # Replace with your actual file path

# Check if your dataframe loaded correctly
print("Dataset shape:", df.shape)
print("Columns:", df.columns.tolist())

# Define your target column (make sure this column exists in your dataset)
target_col = 'aggregate_rating'  # Replace with your actual target column name if different

# Make sure the target column exists
if target_col not in df.columns:
    print(f"Error: Target column '{target_col}' not found in the dataset")
    # Find possible rating columns
    possible_targets = [col for col in df.columns if 'rat' in col.lower() or 'score' in col.lower()]
    if possible_targets:
        print(f"Possible target columns: {possible_targets}")
        target_col = possible_targets[0]  # Use the first possible match
        print(f"Using '{target_col}' as the target column instead")
    else:
        print("No suitable target column found")
        # If you want to continue anyway with a different column:
        # target_col = df.columns[-1]  # Using the last column as a fallback

# Identify feature types
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = df.select_dtypes(include=['object']).columns.tolist()

# Remove target from features
if target_col in numeric_features:
    numeric_features.remove(target_col)
if target_col in categorical_features:
    categorical_features.remove(target_col)

# Print feature counts to verify
print(f"Number of numeric features: {len(numeric_features)}")
print(f"Number of categorical features: {len(categorical_features)}")

# Create the preprocessor
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Split the data
X = df.drop(target_col, axis=1)
y = df[target_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Verify split worked
print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

# Now your models should work
models = {
    'Linear Regression': Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', LinearRegression())
    ]),
    'Decision Tree': Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', DecisionTreeRegressor(random_state=42))
    ]),
    'Random Forest': Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', RandomForestRegressor(random_state=42, n_estimators=100))
    ])
}

# Train and evaluate each model
results = {}

for name, model in models.items():
    print(f"Training {name}...")
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate performance
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results[name] = {
        'mse': mse,
        'r2': r2,
        'model': model
    }

    print(f"{name}:")
    print(f"  Mean Squared Error: {mse:.4f}")
    print(f"  R-squared: {r2:.4f}")
    print("-" * 40)

# Step 3: Evaluate Model Performance

# Find the best model based on MSE
best_model_name = min(results, key=lambda k: results[k]['mse'])
best_model = results[best_model_name]['model']

print(f"Best model: {best_model_name}")
print(f"MSE: {results[best_model_name]['mse']:.4f}")
print(f"R-squared: {results[best_model_name]['r2']:.4f}")

# Make predictions with the best model
y_pred = best_model.predict(X_test)

# Import visualization libraries if not already imported
import matplotlib.pyplot as plt
import seaborn as sns

# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual Rating')
plt.ylabel('Predicted Rating')
plt.title(f'Actual vs Predicted Ratings ({best_model_name})')
plt.tight_layout()
plt.show()

# Plot residuals
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.hlines(y=0, xmin=min(y_pred), xmax=max(y_pred), colors='r', linestyles='--')
plt.xlabel('Predicted Rating')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.tight_layout()
plt.show()

# Calculate and display additional metrics
from sklearn.metrics import mean_absolute_error, median_absolute_error

mae = mean_absolute_error(y_test, y_pred)
medae = median_absolute_error(y_test, y_pred)
rmse = np.sqrt(results[best_model_name]['mse'])

print("\nAdditional Performance Metrics:")
print(f"Mean Absolute Error: {mae:.4f}")
print(f"Median Absolute Error: {medae:.4f}")
print(f"Root Mean Squared Error: {rmse:.4f}")

# Distribution of residuals
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')
plt.axvline(x=0, color='r', linestyle='--')
plt.tight_layout()
plt.show()

# Residuals vs features (for key numeric features)
if len(numeric_features) > 0:
    # Select up to 3 most important numeric features for plotting
    features_to_plot = numeric_features[:min(3, len(numeric_features))]

    fig, axes = plt.subplots(1, len(features_to_plot), figsize=(15, 5))
    if len(features_to_plot) == 1:
        axes = [axes]  # Make axes iterable when there's only one subplot

    for i, feature in enumerate(features_to_plot):
        try:
            axes[i].scatter(X_test[feature], residuals, alpha=0.5)
            axes[i].hlines(y=0, xmin=X_test[feature].min(), xmax=X_test[feature].max(), colors='r', linestyles='--')
            axes[i].set_xlabel(feature)
            axes[i].set_ylabel('Residuals')
            axes[i].set_title(f'Residuals vs {feature}')
        except Exception as e:
            print(f"Could not plot {feature}: {e}")

    plt.tight_layout()
    plt.show()

# Step 4: Feature Importance Analysis

# For Random Forest or Decision Tree - analyze feature importance
if best_model_name in ['Random Forest', 'Decision Tree']:
    try:
        # Get the preprocessor and model from the pipeline
        preprocessor = best_model.named_steps['preprocessor']
        model = best_model.named_steps['regressor']

        # Get feature names after preprocessing
        feature_names = []

        # Handle numerical features
        if len(numeric_features) > 0:
            feature_names.extend(numeric_features)

        # Handle categorical features with one-hot encoding
        if len(categorical_features) > 0:
            for i, col in enumerate(categorical_features):
                try:
                    # Get one-hot encoded feature names for this column
                    encoder = preprocessor.transformers_[1][1].named_steps['onehot']
                    categories = encoder.categories_[i]
                    for cat in categories:
                        feature_names.append(f"{col}_{cat}")
                except:
                    # Fallback if we can't get categories
                    feature_names.append(f"{col}_encoded")

        # Get feature importances from the model
        importances = model.feature_importances_

        # Ensure lengths match (may need adjustment based on your actual preprocessing)
        if len(importances) != len(feature_names):
            print(f"Warning: Feature names length ({len(feature_names)}) doesn't match importances length ({len(importances)})")
            # Use generic feature names as fallback
            feature_names = [f"Feature {i}" for i in range(len(importances))]

        # Create a DataFrame for visualization
        feature_importance = pd.DataFrame({
            'Feature': feature_names[:len(importances)],
            'Importance': importances
        })

        # Sort by importance
        feature_importance = feature_importance.sort_values('Importance', ascending=False)

        # Plot top 15 features (or all if fewer)
        top_n = min(15, len(feature_importance))
        plt.figure(figsize=(12, 8))
        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(top_n))
        plt.title(f'Top {top_n} Features by Importance')
        plt.tight_layout()
        plt.show()

        # Print top features and their importance
        print("\nTop 5 Most Important Features:")
        for idx, row in feature_importance.head(5).iterrows():
            print(f"{row['Feature']}: {row['Importance']:.4f}")

    except Exception as e:
        print(f"Error in feature importance analysis: {e}")

elif best_model_name == 'Linear Regression':
    try:
        # For Linear Regression - analyze coefficients
        model = best_model.named_steps['regressor']

        # Similar logic for feature names
        feature_names = []

        # Add numeric features
        if len(numeric_features) > 0:
            feature_names.extend(numeric_features)

        # Add categorical features (one-hot encoded)
        if len(categorical_features) > 0:
            for i, col in enumerate(categorical_features):
                try:
                    encoder = preprocessor.transformers_[1][1].named_steps['onehot']
                    categories = encoder.categories_[i]
                    for cat in categories:
                        feature_names.append(f"{col}_{cat}")
                except:
                    feature_names.append(f"{col}_encoded")

        # Get coefficients
        coefficients = model.coef_

        # Ensure lengths match
        if len(coefficients) != len(feature_names):
            print(f"Warning: Feature names length ({len(feature_names)}) doesn't match coefficients length ({len(coefficients)})")
            # Use generic feature names as fallback
            feature_names = [f"Feature {i}" for i in range(len(coefficients))]

        # Create a DataFrame with absolute coefficients
        feature_importance = pd.DataFrame({
            'Feature': feature_names[:len(coefficients)],
            'Coefficient': coefficients,
            'Abs_Coefficient': np.abs(coefficients)
        })

        # Sort by absolute coefficient value
        feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)

        # Plot top features
        top_n = min(15, len(feature_importance))
        plt.figure(figsize=(12, 8))
        colors = ['red' if x < 0 else 'blue' for x in feature_importance.head(top_n)['Coefficient']]
        sns.barplot(x='Coefficient', y='Feature', data=feature_importance.head(top_n), palette=colors)
        plt.title(f'Top {top_n} Features by Coefficient Magnitude')
        plt.axvline(x=0, color='black', linestyle='-')
        plt.tight_layout()
        plt.show()

        # Print top coefficients
        print("\nTop 5 Features by Coefficient Magnitude:")
        for idx, row in feature_importance.head(5).iterrows():
            print(f"{row['Feature']}: {row['Coefficient']:.4f}")

    except Exception as e:
        print(f"Error in coefficient analysis: {e}")

# Step 5: Model Interpretation and Summary

print("\n" + "="*50)
print("MODEL SUMMARY REPORT")
print("="*50)

# Summary of model performance
print("\nModel Performance Comparison:")
for name, result in results.items():
    print(f"{name}:")
    print(f"  MSE: {result['mse']:.4f}")
    print(f"  R²: {result['r2']:.4f}")
    print(f"  RMSE: {np.sqrt(result['mse']):.4f}")
    print()

# Interpret R² value
r2 = results[best_model_name]['r2']
if r2 < 0.3:
    r2_interpretation = "poor"
elif r2 < 0.5:
    r2_interpretation = "moderate"
elif r2 < 0.7:
    r2_interpretation = "good"
else:
    r2_interpretation = "excellent"

print(f"\nThe best model ({best_model_name}) explains {r2:.1%} of the variance in restaurant ratings.")
print(f"This indicates {r2_interpretation} predictive performance.")

# Calculate average absolute error in more intuitive terms
if target_col == 'aggregate_rating':
    print(f"\nOn average, the model's predictions are off by {mae:.2f} rating points.")

# Check for overfitting
try:
    # Train performance of best model
    train_pred = best_model.predict(X_train)
    train_r2 = r2_score(y_train, train_pred)

    r2_diff = train_r2 - results[best_model_name]['r2']

    print("\nOverfitting Check:")
    print(f"Training R²: {train_r2:.4f}")
    print(f"Testing R²: {results[best_model_name]['r2']:.4f}")
    print(f"Difference: {r2_diff:.4f}")

    if r2_diff > 0.1:
        print("Warning: Model may be overfitting. Consider regularization or reducing model complexity.")
    else:
        print("Model shows good generalization with minimal overfitting.")
except Exception as e:
    print(f"Could not perform overfitting check: {e}")

# Recommendations
print("\nRecommendations:")
if r2 < 0.5:
    print("1. Consider collecting more data or additional features.")
    print("2. Try more complex models like gradient boosting or neural networks.")
    print("3. Feature engineering might help extract more signal from the existing data.")
else:
    print("1. The model performs well and is ready for deployment.")
    print("2. Consider feature selection to simplify the model if needed.")
    print("3. Regularly retrain the model as new data becomes available.")

print("\nFeature Recommendations:")
print("Based on the analysis, focus on these areas to improve restaurant ratings:")
if best_model_name in ['Random Forest', 'Decision Tree'] and 'feature_importance' in locals():
    for idx, row in feature_importance.head(3).iterrows():
        print(f"- {row['Feature']}")
elif best_model_name == 'Linear Regression' and 'feature_importance' in locals():
    # Positive coefficients
    pos_features = feature_importance[feature_importance['Coefficient'] > 0].head(2)
    neg_features = feature_importance[feature_importance['Coefficient'] < 0].head(2)

    print("Factors that positively impact ratings:")
    for idx, row in pos_features.iterrows():
        print(f"- {row['Feature']}")

    print("\nFactors that negatively impact ratings:")
    for idx, row in neg_features.iterrows():
        print(f"- {row['Feature']}")